{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73fb4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "#for the 2nd part\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "#for the 2nd part\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2469454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the images\n",
    "images = np.load(\"smiley_dataset/smiley_X.npy\")#noiseless dataset\n",
    "images2 = np.load(\"smiley_dataset/smiley_noisy_X.npy\") #noisy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf65915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the classes\n",
    "classes = np.load(\"smiley_dataset/smiley_Y.npy\") #class for noiseless Dataset\n",
    "classes2 = np.load(\"smiley_dataset/smiley_noisy_Y.npy\") #Class for noisy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (204, 9, 9, 1)\n",
      "Images2 shape: (204, 9, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Images shape:\", images.shape) #printing the noiseless dataset\n",
    "print(\"Images2 shape:\", images2.shape)#printing the noisy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc0630",
   "metadata": {},
   "source": [
    "# the shape of the dataset as printed above, tells it is a 4-dimensional numpy array with 204 samples, 9 height, 9 width and 1 channel(implying a grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d1641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(classes)#printing labels for the noiseless dataset\n",
    "print(classes2)#printing labels for the noisy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c747996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0.0 Count: 72\n",
      "Class: 1.0 Count: 60\n",
      "Class: 2.0 Count: 72\n",
      "\n",
      " for noisy dataset \n",
      "\n",
      "Class: 0.0 Count: 72\n",
      "Class: 1.0 Count: 60\n",
      "Class: 2.0 Count: 72\n"
     ]
    }
   ],
   "source": [
    "#verifying the number of unique classes with their corresponding leabels\n",
    "# Get the unique classes and their counts\n",
    "unique_classes, class_counts = np.unique(classes, return_counts=True)\n",
    "# Print the unique classes and their counts\n",
    "for class_label, count in zip(unique_classes, class_counts):\n",
    "    print(\"Class:\", class_label, \"Count:\", count)\n",
    "    \n",
    "print(\"\\n for noisy dataset \\n\")    \n",
    "unique_noisy_classes, noisy_class_counts = np.unique(classes2, return_counts=True)\n",
    "# Print the unique classes and their counts\n",
    "for noisy_class_label, count in zip(unique_noisy_classes, noisy_class_counts):\n",
    "    print(\"Class:\", noisy_class_label, \"Count:\", count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb853db",
   "metadata": {},
   "source": [
    "# the result from the above cell indicates that the both the noisy and noiseless dataset contain 72 images, 60 images and 72 images which belong to sad, neutral and happpy respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f2d53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Selecting some random from the noisy dataset for visualization\n",
    "indices = np.random.choice(len(images2), size=5, replace=False)\n",
    "\n",
    "# Ploting the selected images \n",
    "fig, axes = plt.subplots(1, len(indices), figsize=(10, 4))\n",
    "for i, index in enumerate(indices):\n",
    "    axes[i].imshow(images[index, :, :, 0], cmap='gray')\n",
    "    axes[i].set_title(f\"Class: {classes[index]}\")\n",
    "    axes[i].axis('off')#turning off axis labels for cleaner display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e785bc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Selecting some random indices from the noiseless dataset for visualization\n",
    "indices = np.random.choice(len(images), size=5, replace=False)\n",
    "\n",
    "# Plot the selected images\n",
    "fig, axes = plt.subplots(1, len(indices), figsize=(10, 4))\n",
    "for i, index in enumerate(indices):\n",
    "    axes[i].imshow(images[index, :, :, 0], cmap='gray')\n",
    "    axes[i].set_title(f\"Class: {classes[index]}\")\n",
    "    axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc8801",
   "metadata": {},
   "source": [
    "# Results from the two cell above show visualization of 5 random images from the datasets(noiseless and noisy) which seem to match their corresponding classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea33db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into training and testing sets\n",
    "img_train, img_test, cl_train, cl_test = train_test_split(images, classes, test_size=0.2, random_state=42)\n",
    "img_noisy_train, img_noisy_test, cl_noisy_train, cl_noisy_test = train_test_split(images2, classes2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3786d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the NaÃ¯ve Bayes classifier\n",
    "naive_bayes = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35e746eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the training sets\n",
    "naive_bayes.fit(img_train.reshape(img_train.shape[0], -1), cl_train)\n",
    "naive_bayes.fit(img_noisy_train.reshape(img_noisy_train.shape[0], -1), cl_noisy_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "449f30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the testing set to match the input format\n",
    "img_test_reshaped = img_test.reshape(img_test.shape[0], -1)\n",
    "img_noisy_test_reshaped = img_noisy_test.reshape(img_noisy_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32bea04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "cl_pred = naive_bayes.predict(img_test_reshaped)\n",
    "cl_noisy_pred = naive_bayes.predict(img_noisy_test_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "924d9168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5853658536585366\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(cl_test, cl_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f472d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83f770ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 8  6  1]\n",
      " [ 2  6  1]\n",
      " [ 0  7 10]]\n"
     ]
    }
   ],
   "source": [
    "# Generate and print the confusion matrix\n",
    "confusion_mat = confusion_matrix(cl_test, cl_pred)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d83c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate and print TP rate, FP rate, precision, recall, and F measure\n",
    "tp_rate = confusion_mat[1, 1] / (confusion_mat[1, 1] + confusion_mat[1, 0])\n",
    "fp_rate = confusion_mat[0, 1] / (confusion_mat[0, 1] + confusion_mat[0, 0])\n",
    "precision = confusion_mat[1, 1] / (confusion_mat[1, 1] + confusion_mat[0, 1])\n",
    "recall = tp_rate\n",
    "f_measure = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1402ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP Rate: 0.75\n",
      "FP Rate: 0.42857142857142855\n",
      "Precision: 0.5\n",
      "Recall: 0.75\n",
      "F Measure: 0.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"TP Rate:\", tp_rate)\n",
    "print(\"FP Rate:\", fp_rate)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F Measure:\", f_measure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f4d4ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Area: 0.779325351935646\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print ROC area\n",
    "if len(unique_classes) == 2:  # Binary classification\n",
    "    roc_area = roc_auc_score(cl_test, cl_pred)\n",
    "else:  # Multi-class classification\n",
    "    cl_prob = naive_bayes.predict_proba(img_test_reshaped)\n",
    "    roc_area = roc_auc_score(cl_test, cl_prob, multi_class='ovr')\n",
    "print(\"ROC Area:\", roc_area)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ceffde",
   "metadata": {},
   "source": [
    "# Discusion for the metrics\n",
    "#from the results of the metrics calculated above,\n",
    "Accuracy of 0.5853658536585366 suggest that 58.54 samples were correctly classified\n",
    "True Positive Rate of 0.75 indicates that 75% of the images were identified correctly\n",
    "False Positive Rate of 0.42857142857142855 implies that 42% of the times it avoids misclassifying negative samples as positive\n",
    "precision of 0.5 means that the classifier has mad a 50% positive prediction of correct classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48729f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (204, 9, 9, 1)\n",
      "Classes shape: (204,)\n",
      "Unique Class Labels in 'classes': [0. 1. 2.]\n",
      "Class: 0.0, class_indices: [ 44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61\n",
      "  62  63  64  65  66  67 112 113 114 115 116 117 118 119 120 121 122 123\n",
      " 124 125 126 127 128 129 130 131 132 133 134 135 180 181 182 183 184 185\n",
      " 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203]\n",
      "Class: 1.0, class_indices: [ 24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41\n",
      "  42  43  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 160 161 162 163 164 165 166 167 168 169 170 171 172 173\n",
      " 174 175 176 177 178 179]\n",
      "Class: 2.0, class_indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  68  69  70  71  72  73  74  75  76  77  78  79\n",
      "  80  81  82  83  84  85  86  87  88  89  90  91 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159]\n",
      "Correlation Matrix Shape: (81, 81)\n",
      "Class: 0.0, Number of Samples: 72\n",
      "Error: index 112 is out of bounds for axis 0 with size 81\n",
      "Class: 1.0, Number of Samples: 60\n",
      "Error: index 92 is out of bounds for axis 0 with size 81\n",
      "Class: 2.0, Number of Samples: 72\n",
      "Error: index 81 is out of bounds for axis 0 with size 81\n"
     ]
    }
   ],
   "source": [
    "# Verify the number of samples and the shape of the images and classes arrays\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Classes shape:\", classes.shape)\n",
    "\n",
    "# Verify the unique class labels in the 'classes' array\n",
    "unique_classes_in_classes = np.unique(classes)\n",
    "print(\"Unique Class Labels in 'classes':\", unique_classes_in_classes)\n",
    "\n",
    "# Print 'class_indices' for each class to identify the issue\n",
    "for class_label in unique_classes:\n",
    "    class_indices = np.where(classes == class_label)[0]\n",
    "    print(f\"Class: {class_label}, class_indices:\", class_indices)\n",
    "\n",
    "\n",
    "# Reshape the images into a 2D array (num_samples, num_pixels)\n",
    "num_samples, img_height, img_width, num_channels = images.shape\n",
    "images_reshaped = images.reshape(num_samples, img_height * img_width * num_channels)\n",
    "\n",
    "# Step 1: Identify Correlating Features\n",
    "correlation_matrix = np.corrcoef(images_reshaped, rowvar=False)\n",
    "print(\"Correlation Matrix Shape:\", correlation_matrix.shape)\n",
    "\n",
    "top_features_per_class = []\n",
    "N = 3  # Number of top features to select for each class\n",
    "\n",
    "for class_label in unique_classes:\n",
    "    class_indices = np.where(classes == class_label)[0]\n",
    "    print(f\"Class: {class_label}, Number of Samples: {len(class_indices)}\")\n",
    "\n",
    "    # Check if there are enough samples for the class before computing correlation\n",
    "    if len(class_indices) >= N:\n",
    "        try:\n",
    "            class_corr_values = np.mean(correlation_matrix[class_indices], axis=0)\n",
    "            top_features_indices = np.argsort(class_corr_values)[-N:][::-1]\n",
    "            top_features_per_class.append(top_features_indices)\n",
    "        except IndexError as e:\n",
    "            print(\"Error:\", e)\n",
    "    else:\n",
    "        print(f\"Class {class_label} has insufficient samples for correlation calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ee800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ca8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS THE BEGINING OF THE SECOND PART OF THE ASSIGNMENT\n",
    "#PART TWO BEGINS HERE\n",
    "evaluation_scores = []\n",
    "images_2d = images.reshape(images.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a73f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the KMeans clustering algorithm\n",
    "kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "\n",
    "# Fit the k-means algorithm to the data\n",
    "kmeans.fit(images_2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted cluster labels for the samples\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster centers (centroids)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# Define the image dimensions\n",
    "image_height = 9\n",
    "image_width = 9\n",
    "\n",
    "# Plot the cluster centers\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, center in enumerate(cluster_centers):\n",
    "    plt.subplot(1, len(cluster_centers), i + 1)\n",
    "    plt.imshow(center.reshape((image_height, image_width)), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Cluster {i+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the ARI between the cluster labels and the ground truth labels\n",
    "kmeans_ari = adjusted_rand_score(classes, cluster_labels)\n",
    "print(\"Adjusted Rand Index (ARI):\", kmeans_ari)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e8eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implemeningt NMI for Kmeans clustering\n",
    "kmeans_nmi = normalized_mutual_info_score(classes, cluster_labels)\n",
    "#print the NMI score\n",
    "print(\"NMI SCORe:\", kmeans_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the scores for K-means\n",
    "kmeans_scores = {\n",
    "    \"ARI\": kmeans_ari,\n",
    "    \"NMI\": kmeans_nmi\n",
    "}\n",
    "evaluation_scores.append((\"K-means\", kmeans_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing EM cLustering using GMM\n",
    "\n",
    "# Create an instance of the GaussianMixture algorithm\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "\n",
    "# Fit the GMM to the data\n",
    "gmm.fit(images_2d)\n",
    "\n",
    "# Get the cluster labels\n",
    "gmm_labels = gmm.predict(images_2d)\n",
    "\n",
    "# Calculate the ARI for the GMM clustering\n",
    "gmm_ari = adjusted_rand_score(classes, gmm_labels)\n",
    "\n",
    "# Print the ARI score\n",
    "print(\"Adjusted Rand Index (ARI):\", gmm_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa64560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implemeningt NMI for GMM clustering\n",
    "gmm_nmi = normalized_mutual_info_score(classes, gmm_labels)\n",
    "#print the NMI score\n",
    "print(\"NMI SCORE:\", gmm_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f27d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the scores for GMM\n",
    "gmm_scores = {\n",
    "    \"ARI\": gmm_ari,\n",
    "    \"NMI\": gmm_nmi\n",
    "}\n",
    "evaluation_scores.append((\"GMM\", gmm_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing Hierarchical Clustering\n",
    "# Create an instance of the AgglomerativeClustering algorithm\n",
    "hierarchical = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "# Fit the hierarchical clustering algorithm to the data\n",
    "hierarchical.fit(images_2d)\n",
    "\n",
    "# Get the cluster labels\n",
    "hierarchical_labels = hierarchical.labels_\n",
    "# Calculate the ARI for the hierarchical clustering\n",
    "hierarchical_ari = adjusted_rand_score(classes, hierarchical_labels)\n",
    "print(\"Evaluation Metric Score:\", hierarchical_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implemeningt NMI for HIERARCHICAL clustering\n",
    "hierarchical_nmi = normalized_mutual_info_score(classes, hierarchical_labels)\n",
    "#print the NMI score\n",
    "print(\"NMI SCORE:\", hierarchical_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the scores for Hierarchical clustering\n",
    "hierarchical_scores = {\n",
    "    \"ARI\": hierarchical_ari,\n",
    "    \"NMI\": hierarchical_nmi\n",
    "}\n",
    "evaluation_scores.append((\"Hierarchical\", hierarchical_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fa299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the scores for K-means\n",
    "kmeans_scores = evaluation_scores[0][1]\n",
    "kmeans_ari = kmeans_scores[\"ARI\"]\n",
    "kmeans_nmi = kmeans_scores[\"NMI\"]\n",
    "print(kmeans_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_clusters in range(2,10):\n",
    "    kmeans = KMeans(n_clusters = num_clusters,n_init = 10, random_state = 42)\n",
    "    kmeans.fit(images_2d)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(images_2d, cluster_labels)\n",
    "    kmeans_ari = adjusted_rand_score(classes, cluster_labels)\n",
    "    print(f\"Number of Clusters:{num_clusters}\\t Silhouette Score:{silhouette_avg}\\t ARI:{kmeans_ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58248e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check the number of unique labels\n",
    "num_unique_labels = len(set(classes))\n",
    "if num_unique_labels < 2:\n",
    "    print(\"Error: Number of unique labels is less than 2. Unable to compute evaluation metrics.\")\n",
    "else:\n",
    "    # Loop over different numbers of clusters\n",
    "    for num_clusters in range(2, 10):\n",
    "        # K-means\n",
    "        kmeans = KMeans(n_clusters=num_clusters,n_init=10, random_state=42)\n",
    "        kmeans.fit(images_2d)\n",
    "        kmeans_labels = kmeans.labels_\n",
    "        kmeans_silhouette = silhouette_score(images_2d, kmeans_labels)\n",
    "        kmeans_ari = adjusted_rand_score(classes, kmeans_labels)\n",
    "        kmeans_nmi = normalized_mutual_info_score(classes, kmeans_labels)\n",
    "        print(f\"K-means: Number of Clusters: {num_clusters}\\t Silhouette Score: {kmeans_silhouette}\\t ARI: {kmeans_ari}\\t NMI Score:{kmeans_nmi}\")\n",
    "        \n",
    "        #Em clustering using GMM\n",
    "        gmm = GaussianMixture(n_components = 3, random_state=42)\n",
    "        gmm.fit(images_2d)\n",
    "        gmm_labels = gmm.predict(images_2d)\n",
    "        gmm_silhouette =  silhouette_score(images_2d, gmm_labels)\n",
    "        gmm_ari = adjusted_rand_score(classes, gmm_labels)\n",
    "        print(f\"EM Clustering: Number of Clusters: {num_clusters}\\t Silhouette Score: {gmm_silhouette}\\t ARI: {gmm_ari}\")\n",
    "        \n",
    "        \n",
    "        # Hierarchical clustering\n",
    "        hierarchical = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "        hierarchical.fit(images_2d)\n",
    "        hierarchical_labels = hierarchical.labels_\n",
    "        hierarchical_silhouette = silhouette_score(images_2d, hierarchical_labels)\n",
    "        hierarchical_ari = adjusted_rand_score(classes, hierarchical_labels)\n",
    "        print(f\"Hierarchical Clustering: Number of Clusters: {num_clusters}\\t Silhouette Score: {hierarchical_silhouette}\\t ARI: {hierarchical_ari}\\n \")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of algorithm names and evaluation scores\n",
    "algorithm_names, scores = zip(*evaluation_scores)\n",
    "\n",
    "# Get the evaluation metric names (e.g., ARI, NMI)\n",
    "metric_names = list(evaluation_scores[0][1].keys())\n",
    "\n",
    "# Set the position of the bars on the x-axis\n",
    "x = np.arange(len(algorithm_names))\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.2\n",
    "\n",
    "# Create subplots for each evaluation metric\n",
    "fig, ax = plt.subplots()\n",
    "for i, metric_name in enumerate(metric_names):\n",
    "    # Calculate the position of each bar\n",
    "    pos = x + (i - len(metric_names) / 2) * bar_width\n",
    "\n",
    "    # Get the scores for the current evaluation metric\n",
    "    metric_scores = [score[1][metric_name] for score in evaluation_scores]\n",
    "\n",
    "    # Plot the bars for the current evaluation metric\n",
    "    ax.bar(pos, metric_scores, bar_width, label=metric_name)\n",
    "\n",
    "# Set the x-axis labels and tick positions\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(algorithm_names)\n",
    "plt.xlabel('Clustering Algorithm')\n",
    "plt.ylabel('Evaluation Metric Score')\n",
    "plt.title('Comparison of Clustering Algorithms')\n",
    "\n",
    "# Add a legend\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe860385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check the number of unique labels\n",
    "num_unique_labels = len(set(classes))\n",
    "if num_unique_labels < 2:\n",
    "    print(\"Error: Number of unique labels is less than 2. Unable to compute evaluation metrics.\")\n",
    "else:\n",
    "    # Define the range of clusters to explore\n",
    "    num_clusters_range = range(2, 10)\n",
    "\n",
    "    # Initialize lists to store evaluation metric scores\n",
    "    kmeans_silhouette_scores = []\n",
    "    kmeans_ari_scores = []\n",
    "    kmeans_nmi_scores=[]\n",
    "    gmm_silhouette_scores = []\n",
    "    gmm_ari_scores = []\n",
    "    gmm_nmi_scores=[]\n",
    "    hierarchical_silhouette_scores = []\n",
    "    hierarchical_ari_scores = []\n",
    "    hierarchical_nmi_scores=[]\n",
    "\n",
    "    # Loop over different numbers of clusters\n",
    "    for num_clusters in num_clusters_range:\n",
    "        # K-means\n",
    "        kmeans = KMeans(n_clusters=num_clusters,n_init=10, random_state=42)\n",
    "        kmeans.fit(images_2d)\n",
    "        kmeans_labels = kmeans.labels_\n",
    "        kmeans_silhouette = silhouette_score(images_2d, kmeans_labels)\n",
    "        kmeans_ari = adjusted_rand_score(classes, kmeans_labels)\n",
    "        kmeans_nmi = normalized_mutual_info_score(classes, kmeans_labels)\n",
    "        kmeans_silhouette_scores.append(kmeans_silhouette)\n",
    "        kmeans_ari_scores.append(kmeans_ari)\n",
    "        kmeans_nmi_scores.append(kmeans_nmi)\n",
    "\n",
    "        # EM clustering using GMM\n",
    "        gmm = GaussianMixture(n_components=num_clusters, random_state=42)\n",
    "        gmm.fit(images_2d)\n",
    "        gmm_labels = gmm.predict(images_2d)\n",
    "        gmm_silhouette = silhouette_score(images_2d, gmm_labels)\n",
    "        gmm_ari = adjusted_rand_score(classes, gmm_labels)\n",
    "        gmm_nmi = normalized_mutual_info_score(classes, gmm_labels)\n",
    "        gmm_silhouette_scores.append(gmm_silhouette)\n",
    "        gmm_ari_scores.append(gmm_ari)\n",
    "        gmm_nmi_scores.append(gmm_nmi)\n",
    "\n",
    "        # Hierarchical clustering\n",
    "        hierarchical = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "        hierarchical.fit(images_2d)\n",
    "        hierarchical_labels = hierarchical.labels_\n",
    "        hierarchical_silhouette = silhouette_score(images_2d, hierarchical_labels)\n",
    "        hierarchical_ari = adjusted_rand_score(classes, hierarchical_labels)\n",
    "        hierarchical_nmi = normalized_mutual_info_score(classes, hierarchical_labels)\n",
    "        hierarchical_silhouette_scores.append(hierarchical_silhouette)\n",
    "        hierarchical_ari_scores.append(hierarchical_ari)\n",
    "        hierarchical_nmi_scores.append(hierarchical_nmi)\n",
    "\n",
    "    # Visualize Silhouette Scores\n",
    "    plt.plot(num_clusters_range, kmeans_silhouette_scores, label='K-means')\n",
    "    plt.plot(num_clusters_range, gmm_silhouette_scores, label='GMM')\n",
    "    plt.plot(num_clusters_range, hierarchical_silhouette_scores, label='Hierarchical')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.legend()\n",
    "    plt.title('Silhouette Scores for Different Clustering Algorithms')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize ARI Scores\n",
    "    plt.plot(num_clusters_range, kmeans_ari_scores, label='K-means')\n",
    "    plt.plot(num_clusters_range, gmm_ari_scores, label='GMM')\n",
    "    plt.plot(num_clusters_range, hierarchical_ari_scores, label='Hierarchical')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Adjusted Rand Index (ARI)')\n",
    "    plt.legend()\n",
    "    plt.title('Adjusted Rand Index (ARI) for Different Clustering Algorithms')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize NMI Scores\n",
    "    plt.plot(num_clusters_range, kmeans_nmi_scores, label='K-means')\n",
    "    plt.plot(num_clusters_range, gmm_nmi_scores, label='GMM')\n",
    "    plt.plot(num_clusters_range, hierarchical_nmi_scores, label='Hierarchical')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Normalized Mutual Info (NMI)')\n",
    "    plt.legend()\n",
    "    plt.title('Normalized Mutual Info (NMI) for Different Clustering Algorithms')\n",
    "    plt.show()\n",
    "\n",
    "    # Draw Conclusions\n",
    "    best_kmeans_num_clusters = num_clusters_range[kmeans_silhouette_scores.index(max(kmeans_silhouette_scores))]\n",
    "    best_gmm_num_clusters = num_clusters_range[gmm_silhouette_scores.index(max(gmm_silhouette_scores))]\n",
    "    best_hierarchical_num_clusters = num_clusters_range[hierarchical_silhouette_scores.index(max(hierarchical_silhouette_scores))]\n",
    "    \n",
    "    print(f\"Best number of clusters for K-means: {best_kmeans_num_clusters}\")\n",
    "    print(f\"Best number of clusters for GMM: {best_gmm_num_clusters}\")\n",
    "    print(f\"Best number of clusters for Hierarchical: {best_hierarchical_num_clusters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the 3rd part of the assignment proceeds on the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05566157",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1596126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the input features\n",
    "num_samples, image_height, image_width, num_channels = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(num_samples, image_height * image_width * num_channels)\n",
    "\n",
    "k = 10  # Number of folds for cross-validation\n",
    "scores = cross_val_score(clf, X_train_reshaped, y_train, cv=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy scores for each fold:\")\n",
    "print(scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training set\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "# Reshape the test set\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Create an instance of DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the decision tree classifier to the training data\n",
    "clf.fit(X_train_reshaped, y_train)\n",
    "\n",
    "# Use the fitted classifier to predict the labels for the test set\n",
    "y_pred = clf.predict(X_test_reshaped)\n",
    "\n",
    "# Calculate the accuracy of the classifier's predictions on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e04f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "print(\"Precision:\", precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34172384",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95225c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Experiment with different parameters\n",
    "parameters = [\n",
    "    {\"max_depth\": 3},\n",
    "    {\"min_impurity_decrease\": 0.01},\n",
    "    {\"criterion\": \"entropy\"},\n",
    "    {\"min_samples_leaf\": 5},\n",
    "    {\"ccp_alpha\": 0.1}\n",
    "]\n",
    "\n",
    "# Loop over the parameters\n",
    "for params in parameters:\n",
    "    # Set the parameters for the classifier\n",
    "    clf.set_params(**params)\n",
    "\n",
    "    # Fit the classifier on the training set\n",
    "    clf.fit(X_train_reshaped, y_train)\n",
    "\n",
    "    # Predict on the training set and calculate metrics\n",
    "    y_train_pred = clf.predict(X_train_reshaped)\n",
    "    train_confusion_mat = confusion_matrix(y_train, y_train_pred)\n",
    "    train_TP = train_confusion_mat[1, 1]\n",
    "    train_FP = train_confusion_mat[0, 1]\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
    "    train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "    # Predict on the test set and calculate metrics\n",
    "    y_test_pred = clf.predict(X_test_reshaped)\n",
    "    test_confusion_mat = confusion_matrix(y_test, y_test_pred)\n",
    "    test_TP = test_confusion_mat[1, 1]\n",
    "    test_FP = test_confusion_mat[0, 1]\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Parameters:\", params)\n",
    "    print(\"Training Set:\")\n",
    "    print(\"Accuracy:\", train_accuracy)\n",
    "    print(\"Precision:\", train_precision)\n",
    "    print(\"Recall:\", train_recall)\n",
    "    print(\"TP:\", train_TP)\n",
    "    print(\"FP:\", train_FP)\n",
    "    print(\"F1 Score:\", train_f1)\n",
    "    print(\"Test Set:\")\n",
    "    print(\"Accuracy:\", test_accuracy)\n",
    "    print(\"Precision:\", test_precision)\n",
    "    print(\"Recall:\", test_recall)\n",
    "    print(\"TP:\", test_TP)\n",
    "    print(\"FP:\", test_FP)\n",
    "    print(\"F1 Score:\", test_f1)\n",
    "    print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7372b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following cell contains the requirement for the 5th sub-part of the 3rd prt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848360bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the imput data to have two dimensions\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Create a new training set and testing set with 30% instances in the testing set\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_train_reshaped,y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the decision tree classifier on the new training set\n",
    "clf.fit(X_train_new, y_train_new)\n",
    "\n",
    "# Evaluate accuracy on the training set\n",
    "y_train_pred_new = clf.predict(X_train_new)\n",
    "train_accuracy_new = accuracy_score(y_train_new, y_train_pred_new)\n",
    "\n",
    "# Evaluate accuracy on the testing set\n",
    "y_test_pred_new = clf.predict(X_test_new)\n",
    "test_accuracy_new = accuracy_score(y_test_new, y_test_pred_new)\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracies for 30% instances in testing set:\")\n",
    "print(\"Training Accuracy:\", train_accuracy_new)\n",
    "print(\"Testing Accuracy:\", test_accuracy_new)\n",
    "\n",
    "# Create a new training set and testing set with 60% instances in the testing set\n",
    "X_train_new2, X_test_new2, y_train_new2, y_test_new2 = train_test_split(X_train_reshaped, y_train, test_size=0.6, random_state=42)\n",
    "\n",
    "# Fit the decision tree classifier on the new training set\n",
    "clf.fit(X_train_new2, y_train_new2)\n",
    "\n",
    "# Evaluate accuracy on the training set\n",
    "y_train_pred_new2 = clf.predict(X_train_new2)\n",
    "train_accuracy_new2 = accuracy_score(y_train_new2, y_train_pred_new2)\n",
    "\n",
    "# Evaluate accuracy on the testing set\n",
    "y_test_pred_new2 = clf.predict(X_test_new2)\n",
    "test_accuracy_new2 = accuracy_score(y_test_new2, y_test_pred_new2)\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracies for 60% instances in testing set:\")\n",
    "print(\"Training Accuracy:\", train_accuracy_new2)\n",
    "print(\"Testing Accuracy:\", test_accuracy_new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e80734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#random forest clasifier\n",
    "\n",
    "#reshaping the imput data to have two dimensions\n",
    "images_train_reshaped = images.reshape(images.shape[0], -1)\n",
    "images_test_reshaped = images_test.reshape(images_test.shape[0], -1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_train_reshaped,classes,test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.fit(images_train_reshaped, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7956b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_rf = rf_classifier.predict(images_train_reshaped)\n",
    "train_accuracy_rf = accuracy_score(classes, y_train_pred_rf)\n",
    "train_precision_rf = precision_score(classes, y_train_pred_rf, average='macro')\n",
    "train_recall_rf = recall_score(classes, y_train_pred_rf, average='macro')\n",
    "train_f1_rf = f1_score(classes, y_train_pred_rf, average='macro')\n",
    "# Evaluate other metrics as required\n",
    "\n",
    "y_test_pred_rf = rf_classifier.predict(images_test_reshaped)\n",
    "test_accuracy_rf = accuracy_score(classes, y_test_pred_rf)\n",
    "test_precision_rf = precision_score(y_test, y_test_pred_rf, average='macro')\n",
    "test_recall_rf = recall_score(y_test, y_test_pred_rf, average='macro')\n",
    "test_f1_rf = f1_score(y_test, y_test_pred_rf, average='macro')\n",
    "# Evaluate other metrics as required\n",
    "\n",
    "# Print the results\n",
    "print(\"Random Forest Classifier - Original Training Set:\")\n",
    "print(\"Accuracy:\", train_accuracy_rf)\n",
    "print(\"Precision:\", train_precision_rf)\n",
    "print(\"Recall:\", train_recall_rf)\n",
    "print(\"F1 Score:\", train_f1_rf)\n",
    "\n",
    "print(\"Random Forest Classifier - Test Set:\")\n",
    "print(\"Accuracy:\", test_accuracy_rf)\n",
    "print(\"Precision:\", test_precision_rf)\n",
    "print(\"Recall:\", test_recall_rf)\n",
    "print(\"F1 Score:\", test_f1_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba34d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reshape the input data to have two dimensions\n",
    "images_train_reshaped = images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_train_reshaped, classes, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set and calculate metrics\n",
    "y_train_pred_rf = rf_classifier.predict(X_train)\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "train_precision_rf = precision_score(y_train, y_train_pred_rf, average='macro')\n",
    "train_recall_rf = recall_score(y_train, y_train_pred_rf, average='macro')\n",
    "train_f1_rf = f1_score(y_train, y_train_pred_rf, average='macro')\n",
    "train_confusion_matrix_rf = confusion_matrix(y_train, y_train_pred_rf)\n",
    "train_tp_rf = train_confusion_matrix_rf.diagonal()\n",
    "train_fp_rf = train_confusion_matrix_rf.sum(axis=0) - train_tp_rf\n",
    "\n",
    "# Predict on the test set and calculate metrics\n",
    "y_test_pred_rf = rf_classifier.predict(X_test)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "test_precision_rf = precision_score(y_test, y_test_pred_rf, average='macro')\n",
    "test_recall_rf = recall_score(y_test, y_test_pred_rf, average='macro')\n",
    "test_f1_rf = f1_score(y_test, y_test_pred_rf, average='macro')\n",
    "test_confusion_matrix_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "test_tp_rf = test_confusion_matrix_rf.diagonal()\n",
    "test_fp_rf = test_confusion_matrix_rf.sum(axis=0) - test_tp_rf\n",
    "\n",
    "# Print the results\n",
    "print(\"Random Forest Classifier - Training Set:\")\n",
    "print(\"Accuracy:\", train_accuracy_rf)\n",
    "print(\"Precision:\", train_precision_rf)\n",
    "print(\"Recall:\", train_recall_rf)\n",
    "print(\"F1 Score:\", train_f1_rf)\n",
    "print(\"TP:\", train_tp_rf)\n",
    "print(\"FP:\", train_fp_rf)\n",
    "\n",
    "print(\"Random Forest Classifier - Test Set:\")\n",
    "print(\"Accuracy:\", test_accuracy_rf)\n",
    "print(\"Precision:\", test_precision_rf)\n",
    "print(\"Recall:\", test_recall_rf)\n",
    "print(\"F1 Score:\", test_f1_rf)\n",
    "print(\"TP:\", test_tp_rf)\n",
    "print(\"FP:\", test_fp_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part is for the 4th part of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f144ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Reshape the input data to have two dimensions\n",
    "images_train_reshaped = images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_train_reshaped, classes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0766b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Linear classifier\n",
    "linear_clf = LinearRegression()\n",
    "\n",
    "# Fit the classifier on the training set\n",
    "linear_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set and calculate accuracy\n",
    "y_train_pred_linear = linear_clf.predict(X_train)\n",
    "train_accuracy_linear = accuracy_score(y_train, y_train_pred_linear.round())\n",
    "\n",
    "# Predict on the test set and calculate accuracy\n",
    "y_test_pred_linear = linear_clf.predict(X_test)\n",
    "test_accuracy_linear = accuracy_score(y_test, y_test_pred_linear.round())\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Linear Classifier - Training Set (without cross-validation):\")\n",
    "print(\"Accuracy:\", train_accuracy_linear)\n",
    "\n",
    "print(\"Linear Classifier - Test Set (without cross-validation):\")\n",
    "print(\"Accuracy:\", test_accuracy_linear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7235e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Linear classifier\n",
    "linear_clf_cv = LinearRegression()\n",
    "\n",
    "# Perform cross-validation and calculate the accuracies\n",
    "cv_scores = cross_val_score(linear_clf_cv, X_train, y_train, cv=10)\n",
    "cv_accuracy_mean = cv_scores.mean()\n",
    "\n",
    "# Print the cross-validation accuracy\n",
    "print(\"Linear Classifier - Cross-Validation Accuracy:\")\n",
    "print(\"Mean Accuracy:\", cv_accuracy_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1023582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize the MLP classifier\n",
    "mlp_clf = MLPClassifier(max_iter =500)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'hidden_layer_sizes': [(100,), (50, 50), (100, 50, 100)],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation to find the best parameters\n",
    "grid_search = GridSearchCV(mlp_clf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best MLP classifier\n",
    "best_params = grid_search.best_params_\n",
    "best_mlp_clf = grid_search.best_estimator_\n",
    "\n",
    "# Fit the best MLP classifier on the training set\n",
    "best_mlp_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set and calculate accuracy\n",
    "y_train_pred_mlp = best_mlp_clf.predict(X_train)\n",
    "train_accuracy_mlp = accuracy_score(y_train, y_train_pred_mlp)\n",
    "\n",
    "# Predict on the test set and calculate accuracy\n",
    "y_test_pred_mlp = best_mlp_clf.predict(X_test)\n",
    "test_accuracy_mlp = accuracy_score(y_test, y_test_pred_mlp)\n",
    "\n",
    "# Print the results\n",
    "print(\"Multilayer Perceptron (MLP) - Best Parameters:\")\n",
    "print(best_params)\n",
    "\n",
    "print(\"Multilayer Perceptron (MLP) - Training Set:\")\n",
    "print(\"Accuracy:\", train_accuracy_mlp)\n",
    "\n",
    "print(\"Multilayer Perceptron (MLP) - Test Set:\")\n",
    "print(\"Accuracy:\", test_accuracy_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "#Defining the image size\n",
    "img_width = 100\n",
    "img_height = 100\n",
    "img_channels = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Resize the images to (100, 100)\n",
    "images_train_resized = [resize(img, (100, 100)) for img in images_train]\n",
    "\n",
    "# Convert the resized images to a numpy array and add a channel dimension\n",
    "images_train_reshaped = np.array(images_train_resized).reshape(-1, 100, 100, 1)\n",
    "\n",
    "\n",
    "print(\"train image reshaped size\",images_train_reshaped.shape)\n",
    "print(\"test image reshaped size\",images_test_reshaped.shape)\n",
    "\n",
    "images_train_reshaped = images_train_reshaped.reshape(-1, 100, 100, 1)\n",
    "images_test_reshaped = images_test_reshaped.reshape(-1, 100, 100, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_train_categorical = classes\n",
    "# Define and compile the CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, img_channels)))\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Train the CNN model\n",
    "cnn_model.fit(images_train_reshaped, y_train_categorical, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the CNN model on the test set\n",
    "test_loss, test_accuracy_cnn = cnn_model.evaluate(images_test_reshaped, y_test_categorical)\n",
    "\n",
    "# Print the test accuracy for CNN\n",
    "print(\"Convolutional Neural Network (CNN) - Test Set Accuracy:\")\n",
    "print(\"Accuracy:\", test_accuracy_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22034e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eef4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
